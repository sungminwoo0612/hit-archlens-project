{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM 기반 자동 라벨링\n",
        "\nGPT-4 Vision을 사용한 LLM 기반 라벨링\n",
        "\n**원본 스크립트**: `auto_label_aws.sh`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "#!/usr/bin/env bash\n",
        "set -euo pipefail\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## =========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Auto Labeling for AWS Icons in Architecture Diagrams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## - Modes:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A) full_image_llm : Whole-image -> LLM detects (bboxes + classes) in one shot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B) patch_llm      : Region proposals -> LLM suggests names -> taxonomy normalize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## - Outputs: YOLO TXT, COCO JSON, Label Studio JSON\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## =========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --------- User Config (quick toggles) ----------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "PROJECT_DIR=\"${PWD}/aws_llm_autolabel\"\n",
        "IMAGES_DIR=\"${PROJECT_DIR}/images\"\n",
        "URL_LIST=\"${PROJECT_DIR}/images.txt\"            # optional: image URLs\n",
        "TAXONOMY_CSV=\"/mnt/data/aws_resources_models.csv\"  # provided by user\n",
        "MODE=\"${MODE:-full_image_llm}\"                  # full_image_llm | patch_llm\n",
        "PROVIDER=\"${PROVIDER:-openai}\"                  # openai | deepseek\n",
        "OPENAI_MODEL_VISION=\"${OPENAI_MODEL_VISION:-gpt-4o-mini}\"   # or gpt-4.1, gpt-4o\n",
        "OPENAI_MODEL_EMBED=\"${OPENAI_MODEL_EMBED:-text-embedding-3-large}\"\n",
        "DEEPSEEK_MODEL_VISION=\"${DEEPSEEK_MODEL_VISION:-deepseek-vl-1.5}\"  # example\n",
        "MAX_WORKERS=\"${MAX_WORKERS:-4}\"\n",
        "OUT_FORMAT=\"${OUT_FORMAT:-labelstudio}\"         # yolo | coco | labelstudio\n",
        "CONF_THRESHOLD=\"${CONF_THRESHOLD:-0.35}\"\n",
        "echo \"[+] Project: ${PROJECT_DIR}\"\n",
        "mkdir -p \"${PROJECT_DIR}/src\" \"${PROJECT_DIR}/out\" \"${IMAGES_DIR}\"\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --------- Python env ----------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "if ! command -v python3 >/dev/null 2>&1; then\n",
        "  echo \"Python3가 필요합니다.\"; exit 1\n",
        "fi\n",
        "python3 -m venv \"${PROJECT_DIR}/.venv\"\n",
        "source \"${PROJECT_DIR}/.venv/bin/activate\"\n",
        "pip install -U pip >/dev/null\n",
        "pip install \\\n",
        "  openai \\\n",
        "  requests \\\n",
        "  numpy \\\n",
        "  pandas \\\n",
        "  pillow \\\n",
        "  opencv-python-headless \\\n",
        "  tqdm \\\n",
        "  rapidfuzz \\\n",
        "  pydantic \\\n",
        "  pyyaml >/dev/null\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --------- Config ----------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${PROJECT_DIR}/config.yaml\" <<'YAML'\n",
        "project_name: aws_llm_autolabel\n",
        "provider: ${PROVIDER}         # openai | deepseek (env-var expanded at runtime)\n",
        "mode: ${MODE}                  # full_image_llm | patch_llm\n",
        "openai:\n",
        "  vision_model: ${OPENAI_MODEL_VISION}\n",
        "  embed_model: ${OPENAI_MODEL_EMBED}\n",
        "deepseek:\n",
        "  vision_model: ${DEEPSEEK_MODEL_VISION}\n",
        "data:\n",
        "  images_dir: ./images\n",
        "  url_list: ./images.txt       # optional\n",
        "  taxonomy_csv: /mnt/data/aws_resources_models.csv\n",
        "runtime:\n",
        "  max_workers: ${MAX_WORKERS}\n",
        "  conf_threshold: ${CONF_THRESHOLD}\n",
        "output:\n",
        "  dir: ./out\n",
        "  format: ${OUT_FORMAT}        # yolo | coco | labelstudio\n",
        "YAML\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --------- Core: taxonomy loader & normalizer ----------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${PROJECT_DIR}/src/taxonomy.py\" <<'PY'\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "from rapidfuzz import process, fuzz\n",
        "@dataclass\n",
        "class Taxonomy:\n",
        "    canonical_to_aliases: Dict[str, List[str]]\n",
        "    alias_to_canonical: Dict[str, str]\n",
        "    names: List[str]\n",
        "    @classmethod\n",
        "    def from_csv(cls, path: str) -> \"Taxonomy\":\n",
        "        \"\"\"\n",
        "        Expect CSV columns like:\n",
        "          canonical,aliases\n",
        "        where aliases is '|' separated optional list\n",
        "        If your CSV has different columns (e.g., 'service','family', etc),\n",
        "        tweak the parsing below accordingly.\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(path)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # Heuristics: find best columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "        cols = [c.lower() for c in df.columns]\n",
        "        try:\n",
        "            name_col = next(c for c in df.columns if c.lower() in (\"canonical\",\"name\",\"service\",\"label\"))\n",
        "        except StopIteration:\n",
        "            name_col = df.columns[0]\n",
        "        alias_col = None\n",
        "        for c in df.columns:\n",
        "            if c.lower() in (\"aliases\",\"alias\",\"aka\"):\n",
        "                alias_col = c\n",
        "                break\n",
        "        canonical_to_aliases = {}\n",
        "        alias_to_canonical = {}\n",
        "        for _, row in df.iterrows():\n",
        "            canon = str(row[name_col]).strip()\n",
        "            aliases = []\n",
        "            if alias_col and not pd.isna(row[alias_col]):\n",
        "                aliases = [a.strip() for a in str(row[alias_col]).split(\"|\") if a.strip()]\n",
        "            all_keys = set([canon] + aliases)\n",
        "            canonical_to_aliases[canon] = list(all_keys)\n",
        "            for k in all_keys:\n",
        "                alias_to_canonical[k.lower()] = canon\n",
        "        names = list(canonical_to_aliases.keys())\n",
        "        return cls(canonical_to_aliases, alias_to_canonical, names)\n",
        "    def normalize(self, s: str) -> Tuple[str, float]:\n",
        "        \"\"\"Map arbitrary text to canonical via fuzzy matching + alias map.\"\"\"\n",
        "        key = s.strip().lower()\n",
        "        if key in self.alias_to_canonical:\n",
        "            return self.alias_to_canonical[key], 1.0\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # fuzzy to all aliases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "        best = process.extractOne(\n",
        "            key,\n",
        "            list(self.alias_to_canonical.keys()),\n",
        "            scorer=fuzz.WRatio\n",
        "        )\n",
        "        if best:\n",
        "            alias, score, _ = best\n",
        "            return self.alias_to_canonical[alias], score/100.0\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # fallback to canonical names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "        best2 = process.extractOne(\n",
        "            key, self.names, scorer=fuzz.WRatio\n",
        "        )\n",
        "        if best2:\n",
        "            name, score, _ = best2\n",
        "            return name, score/100.0\n",
        "        return s, 0.0\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --------- Simple region proposals (for patch_llm) ----------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${PROJECT_DIR}/src/proposals.py\" <<'PY'\n",
        "import cv2, numpy as np\n",
        "from typing import List, Tuple\n",
        "def sliding_windows(h, w, win=96, stride=64):\n",
        "    for y in range(0, max(1, h - win), stride):\n",
        "        for x in range(0, max(1, w - win), stride):\n",
        "            yield x, y, win, win\n",
        "def contour_proposals(img, min_area=800, max_area=50000):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    e = cv2.Canny(gray, 50, 150)\n",
        "    cnts, _ = cv2.findContours(e, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    boxes = []\n",
        "    for c in cnts:\n",
        "        x,y,w,h = cv2.boundingRect(c)\n",
        "        area = w*h\n",
        "        if min_area <= area <= max_area:\n",
        "            boxes.append((x,y,w,h))\n",
        "    return boxes\n",
        "def propose_regions(img_bgr) -> List[Tuple[int,int,int,int]]:\n",
        "    h, w = img_bgr.shape[:2]\n",
        "    boxes = list(sliding_windows(h, w))  # coarse proposals\n",
        "    boxes += contour_proposals(img_bgr)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # optional: NMS to reduce overlaps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    return boxes\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --------- LLM calls (OpenAI / DeepSeek) ----------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${PROJECT_DIR}/src/llm.py\" <<'PY'\n",
        "import base64, io, os, json, time\n",
        "from typing import List, Dict, Any, Optional\n",
        "from PIL import Image\n",
        "import requests\n",
        "OPENAI_BASE = os.getenv(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\")\n",
        "DEEPSEEK_BASE = os.getenv(\"DEEPSEEK_BASE_URL\", \"https://api.deepseek.com\")\n",
        "def pil_to_b64(pil_img: Image.Image) -> str:\n",
        "    buf = io.BytesIO()\n",
        "    pil_img.save(buf, format=\"PNG\")\n",
        "    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
        "def call_openai_vision(image_pil: Image.Image, prompt: str, model: str) -> str:\n",
        "    import openai\n",
        "    client = openai.OpenAI(base_url=OPENAI_BASE, api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    b64 = pil_to_b64(image_pil)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\":\"You are a precise vision annotator. Return strictly valid JSON.\"},\n",
        "            {\"role\":\"user\",\"content\":[\n",
        "                {\"type\":\"text\",\"text\":prompt},\n",
        "                {\"type\":\"image_url\",\"image_url\":{\"url\": f\"data:image/png;base64,{b64}\"}}\n",
        "            ]}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "def call_deepseek_vision(image_pil: Image.Image, prompt: str, model: str) -> str:\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # Generic VL endpoint; adjust to DeepSeek's latest if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    b64 = pil_to_b64(image_pil)\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {os.getenv('DEEPSEEK_API_KEY')}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":\"You are a precise vision annotator. Return strictly valid JSON.\"},\n",
        "            {\"role\":\"user\",\"content\":[\n",
        "                {\"type\":\"text\",\"text\":prompt},\n",
        "                {\"type\":\"image_url\",\"image_url\":{\"url\": f\"data:image/png;base64,{b64}\"}}\n",
        "            ]}\n",
        "        ],\n",
        "        \"temperature\": 0\n",
        "    }\n",
        "    r = requests.post(f\"{DEEPSEEK_BASE}/chat/completions\", headers=headers, json=payload, timeout=120)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --------- Embedding helper (OpenAI only; optional) ----------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${PROJECT_DIR}/src/embeds.py\" <<'PY'\n",
        "import os\n",
        "from typing import List\n",
        "def embed_texts(texts: List[str], model: str) -> List[List[float]]:\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # Optional refinement step; if no OPENAI key, skip in caller.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    import openai\n",
        "    client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    out = client.embeddings.create(model=model, input=texts)\n",
        "    return [d.embedding for d in out.data]\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --------- IO utils ----------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${PROJECT_DIR}/src/io_utils.py\" <<'PY'\n",
        "import os, json, hashlib, shutil\n",
        "from typing import List, Dict, Any\n",
        "from PIL import Image\n",
        "import requests\n",
        "def ensure_dir(p): os.makedirs(p, exist_ok=True)\n",
        "def download_images(url_list_path: str, out_dir: str):\n",
        "    ensure_dir(out_dir)\n",
        "    if not os.path.exists(url_list_path): return\n",
        "    with open(url_list_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            url = line.strip()\n",
        "            if not url: continue\n",
        "            h = hashlib.md5(url.encode()).hexdigest()[:10]\n",
        "            fn = os.path.join(out_dir, f\"{h}.png\")\n",
        "            if os.path.exists(fn): continue\n",
        "            r = requests.get(url, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            with open(fn, \"wb\") as w:\n",
        "                w.write(r.content)\n",
        "def list_images(dir_path: str) -> List[str]:\n",
        "    exts = (\".png\",\".jpg\",\".jpeg\",\".webp\",\".bmp\")\n",
        "    files = []\n",
        "    for root, _, fs in os.walk(dir_path):\n",
        "        for f in fs:\n",
        "            if f.lower().endswith(exts):\n",
        "                files.append(os.path.join(root, f))\n",
        "    return sorted(files)\n",
        "def load_image(path: str) -> Image.Image:\n",
        "    return Image.open(path).convert(\"RGB\")\n",
        "def save_json(path: str, data: Any):\n",
        "    with open(path,\"w\",encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --------- Annotation exporters ----------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${PROJECT_DIR}/src/exporters.py\" <<'PY'\n",
        "import os, json\n",
        "from typing import List, Dict, Any\n",
        "from io import BytesIO\n",
        "def to_labelstudio(items: List[dict]) -> dict:\n",
        "    \"\"\"\n",
        "    items: list of dict per image\n",
        "      { \"image_path\":..., \"width\":..., \"height\":...,\n",
        "        \"objects\":[ {\"bbox\":[x,y,w,h], \"label\": \"Amazon S3\", \"score\":0.88}, ...] }\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for it in items:\n",
        "        img_rel = os.path.basename(it[\"image_path\"])\n",
        "        anns = []\n",
        "        for obj in it.get(\"objects\", []):\n",
        "            x,y,w,h = obj[\"bbox\"]\n",
        "            anns.append({\n",
        "                \"from_name\":\"label\",\n",
        "                \"to_name\":\"image\",\n",
        "                \"type\":\"rectanglelabels\",\n",
        "                \"value\":{\n",
        "                    \"x\": x/it[\"width\"]*100,\n",
        "                    \"y\": y/it[\"height\"]*100,\n",
        "                    \"width\": w/it[\"width\"]*100,\n",
        "                    \"height\": h/it[\"height\"]*100,\n",
        "                    \"rectanglelabels\":[obj[\"label\"]],\n",
        "                    \"score\": obj.get(\"score\", None)\n",
        "                }\n",
        "            })\n",
        "        out.append({\n",
        "            \"data\":{\"image\": img_rel},\n",
        "            \"annotations\":[{\"result\": anns}]\n",
        "        })\n",
        "    return out\n",
        "def to_yolo(items: List[dict], out_dir: str):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    name_to_id = {}\n",
        "    next_id = 0\n",
        "    for it in items:\n",
        "        w, h = it[\"width\"], it[\"height\"]\n",
        "        label_path = os.path.join(out_dir, os.path.splitext(os.path.basename(it[\"image_path\"]))[0] + \".txt\")\n",
        "        lines = []\n",
        "        for obj in it.get(\"objects\", []):\n",
        "            x,y,bw,bh = obj[\"bbox\"]\n",
        "            cx = (x + bw/2)/w\n",
        "            cy = (y + bh/2)/h\n",
        "            ww = bw/w\n",
        "            hh = bh/h\n",
        "            name = obj[\"label\"]\n",
        "            if name not in name_to_id:\n",
        "                name_to_id[name] = next_id; next_id += 1\n",
        "            cls_id = name_to_id[name]\n",
        "            lines.append(f\"{cls_id} {cx:.6f} {cy:.6f} {ww:.6f} {hh:.6f}\")\n",
        "        with open(label_path,\"w\") as f:\n",
        "            f.write(\"\\n\".join(lines))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # also export classes.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    with open(os.path.join(out_dir,\"classes.txt\"),\"w\") as f:\n",
        "        for name,_id in sorted(name_to_id.items(), key=lambda x:x[1]):\n",
        "            f.write(f\"{name}\\n\")\n",
        "def to_coco(items: List[dict]) -> dict:\n",
        "    images, annotations, categories = [], [], []\n",
        "    name_to_id = {}\n",
        "    for img_id, it in enumerate(items, start=1):\n",
        "        images.append({\n",
        "            \"id\": img_id,\n",
        "            \"file_name\": os.path.basename(it[\"image_path\"]),\n",
        "            \"width\": it[\"width\"],\n",
        "            \"height\": it[\"height\"]\n",
        "        })\n",
        "        for ann_id, obj in enumerate(it.get(\"objects\", []), start=len(annotations)+1):\n",
        "            name = obj[\"label\"]\n",
        "            if name not in name_to_id:\n",
        "                name_to_id[name] = len(name_to_id)+1\n",
        "            cid = name_to_id[name]\n",
        "            x,y,w,h = obj[\"bbox\"]\n",
        "            annotations.append({\n",
        "                \"id\": ann_id,\n",
        "                \"image_id\": img_id,\n",
        "                \"category_id\": cid,\n",
        "                \"bbox\": [x,y,w,h],\n",
        "                \"area\": w*h,\n",
        "                \"iscrowd\": 0,\n",
        "                \"score\": obj.get(\"score\", None)\n",
        "            })\n",
        "    for name, cid in name_to_id.items():\n",
        "        categories.append({\"id\": cid, \"name\": name})\n",
        "    return {\"images\": images, \"annotations\": annotations, \"categories\": categories}\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --------- Main runner ----------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${PROJECT_DIR}/src/run.py\" <<'PY'\n",
        "import os, json, yaml, math\n",
        "from typing import List, Dict, Any\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from taxonomy import Taxonomy\n",
        "from io_utils import download_images, list_images, load_image, save_json\n",
        "from exporters import to_labelstudio, to_yolo, to_coco\n",
        "from llm import call_openai_vision, call_deepseek_vision\n",
        "from proposals import propose_regions\n",
        "def prompt_full_image():\n",
        "    return \"\"\"Detect AWS service icons on this architecture diagram.\n",
        "Return STRICT JSON:\n",
        "{\n",
        " \"objects\":[\n",
        "   {\"name\":\"<service-name>\", \"bbox\":[x,y,w,h], \"confidence\":0.0}\n",
        " ]\n",
        "}\n",
        "Coordinates in pixels, integer. Only include real icons.\n",
        "For service-name, use the product or service brand as seen or inferred (e.g., \"Amazon S3\", \"AWS Lambda\").\"\"\"\n",
        "def prompt_patch():\n",
        "    return \"\"\"Identify the SINGLE most likely AWS service or product for this small icon patch.\n",
        "Return STRICT JSON: {\"candidates\": [\"<name1>\", \"<name2>\", \"<name3>\"]} (max 3).\"\"\"\n",
        "def choose_provider_call(provider: str, image: Image.Image, prompt: str, models: dict) -> str:\n",
        "    if provider == \"openai\":\n",
        "        return call_openai_vision(image, prompt, models[\"openai\"][\"vision_model\"])\n",
        "    elif provider == \"deepseek\":\n",
        "        return call_deepseek_vision(image, prompt, models[\"deepseek\"][\"vision_model\"])\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown provider: {provider}\")\n",
        "def safe_json(s: str) -> dict:\n",
        "    import json, re\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # try direct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    try: return json.loads(s)\n",
        "    except: pass\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # crude bracket extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    m = None\n",
        "    for br in [\"{\", \"[\"]:\n",
        "        start = s.find(br)\n",
        "        if start >= 0:\n",
        "            try:\n",
        "                return json.loads(s[start:])\n",
        "            except:\n",
        "                continue\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # last resort\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    return {\"objects\": []}\n",
        "def run_full_image(cfg, tax: Taxonomy, paths: List[str]) -> List[dict]:\n",
        "    provider = cfg[\"provider\"]\n",
        "    models = cfg\n",
        "    out = []\n",
        "    for p in tqdm(paths, desc=\"Full-image LLM\"):\n",
        "        im = load_image(p)\n",
        "        rsp = choose_provider_call(provider, im, prompt_full_image(), models)\n",
        "        data = safe_json(rsp)\n",
        "        W, H = im.size\n",
        "        objs = []\n",
        "        for o in data.get(\"objects\", []):\n",
        "            name = str(o.get(\"name\",\"\")).strip()\n",
        "            bbox = o.get(\"bbox\", [0,0,0,0])\n",
        "            conf = float(o.get(\"confidence\", 0.0))\n",
        "            if conf < float(cfg[\"runtime\"][\"conf_threshold\"]): \n",
        "                continue\n",
        "            canon, score = tax.normalize(name)\n",
        "            objs.append({\n",
        "                \"bbox\": [int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])],\n",
        "                \"label\": canon,\n",
        "                \"score\": round(min(conf, score), 4)\n",
        "            })\n",
        "        out.append({\"image_path\": p, \"width\": W, \"height\": H, \"objects\": objs})\n",
        "    return out\n",
        "def run_patch_llm(cfg, tax: Taxonomy, paths: List[str]) -> List[dict]:\n",
        "    provider = cfg[\"provider\"]\n",
        "    models = cfg\n",
        "    out = []\n",
        "    for p in tqdm(paths, desc=\"Patch-LLM\"):\n",
        "        im = load_image(p)\n",
        "        W, H = im.size\n",
        "        import cv2, numpy as np\n",
        "        import numpy as _np\n",
        "        img_bgr = cv2.cvtColor(_np.array(im), cv2.COLOR_RGB2BGR)\n",
        "        boxes = propose_regions(img_bgr)\n",
        "        objs = []\n",
        "        for (x,y,w,h) in boxes:\n",
        "            crop = im.crop((x,y,x+w,y+h))\n",
        "            rsp = choose_provider_call(provider, crop, prompt_patch(), models)\n",
        "            data = safe_json(rsp)\n",
        "            cands = data.get(\"candidates\", [])\n",
        "            best_label, best_score = None, 0.0\n",
        "            for cand in cands:\n",
        "                canon, sc = tax.normalize(str(cand))\n",
        "                if sc > best_score:\n",
        "                    best_label, best_score = canon, sc\n",
        "            if best_label and best_score >= 0.5:\n",
        "                objs.append({\"bbox\":[x,y,w,h], \"label\": best_label, \"score\": round(best_score,3)})\n",
        "        out.append({\"image_path\": p, \"width\": W, \"height\": H, \"objects\": objs})\n",
        "    return out\n",
        "def main():\n",
        "    with open(\"config.yaml\",\"r\",encoding=\"utf-8\") as f:\n",
        "        raw = f.read()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # allow env expansion like ${PROVIDER}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    import os\n",
        "    raw = os.path.expandvars(raw)\n",
        "    cfg = yaml.safe_load(raw)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # Download if url list exists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    download_images(cfg[\"data\"][\"url_list\"], cfg[\"data\"][\"images_dir\"])\n",
        "    paths = list_images(cfg[\"data\"][\"images_dir\"])\n",
        "    if not paths:\n",
        "        print(\"No images found. Put files in ./images or fill images.txt.\"); return\n",
        "    tax = Taxonomy.from_csv(cfg[\"data\"][\"taxonomy_csv\"])\n",
        "    if cfg[\"mode\"] == \"full_image_llm\":\n",
        "        items = run_full_image(cfg, tax, paths)\n",
        "    else:\n",
        "        items = run_patch_llm(cfg, tax, paths)\n",
        "    os.makedirs(cfg[\"output\"][\"dir\"], exist_ok=True)\n",
        "    if cfg[\"output\"][\"format\"] == \"yolo\":\n",
        "        to_yolo(items, os.path.join(cfg[\"output\"][\"dir\"], \"yolo\"))\n",
        "    elif cfg[\"output\"][\"format\"] == \"coco\":\n",
        "        save_json(os.path.join(cfg[\"output\"][\"dir\"], \"coco.json\"), to_coco(items))\n",
        "    else:\n",
        "        save_json(os.path.join(cfg[\"output\"][\"dir\"], \"labelstudio.json\"), to_labelstudio(items))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # Save raw too\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    save_json(os.path.join(cfg[\"output\"][\"dir\"], \"raw_items.json\"), items)\n",
        "    print(\"[Done] Output saved to\", cfg[\"output\"][\"dir\"])\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --------- Helper: tiny demo images.txt (placeholder) ----------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "if [ ! -f \"${URL_LIST}\" ]; then\n",
        "  cat > \"${URL_LIST}\" <<'TXT'\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 여기에 다이어그램 이미지 URL을 줄바꿈으로 추가하세요. (png/jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 예시:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## https://raw.githubusercontent.com/somewhere/sample-aws-arch.png\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "TXT\n",
        "fi\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --------- How to run ----------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat <<'EOM'\n",
        "========================================\n",
        "[사용법]\n",
        "1) OpenAI 또는 DeepSeek API 키를 환경변수로 설정 (택1 또는 둘 다)\n",
        "   export OPENAI_API_KEY=sk-...\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # 또는\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "   export DEEPSEEK_API_KEY=...\n",
        "2) 이미지 준비\n",
        "   - 이미지 파일을 aws_llm_autolabel/images/ 에 넣거나\n",
        "   - aws_llm_autolabel/images.txt 에 URL 목록을 작성\n",
        "3) 모드/출력 포맷 선택(옵션)\n",
        "   export MODE=full_image_llm   # 빠른 데모 (기본)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # 또는\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "   export MODE=patch_llm        # 후보영역→정규화(정확도↑)\n",
        "   export OUT_FORMAT=labelstudio # 기본 (labelstudio|yolo|coco)\n",
        "   export PROVIDER=openai        # 기본 (openai|deepseek)\n",
        "4) 실행\n",
        "   cd aws_llm_autolabel\n",
        "   source .venv/bin/activate\n",
        "   python src/run.py\n",
        "5) 결과\n",
        "   - ./out/labelstudio.json (Label Studio import)\n",
        "   - ./out/yolo/*          (YOLO 라벨)\n",
        "   - ./out/coco.json       (COCO)\n",
        "[팁]\n",
        "- full_image_llm이 월요일 시연에 가장 빠릅니다.\n",
        "- 정확도 보완이 필요하면 patch_llm로 한 번 더 돌려 병합하거나,\n",
        "  labelstudio.json을 Label Studio에 불러 수동 검수(반자동) 후 Export 하세요.\n",
        "- taxonomy CSV의 열 이름이 다르다면 src/taxonomy.py 내 파싱 로직을 간단히 수정하세요.\n",
        "========================================\n",
        "EOM\n"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}