{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CV 기반 자동 라벨링\n",
        "\nCLIP 모델을 사용한 Computer Vision 기반 라벨링\n",
        "\n**원본 스크립트**: `auto_aws_cv_clip.sh`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "#!/usr/bin/env bash\n",
        "set -euo pipefail\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ============ Paths ============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "ROOT=\"${PWD}/aws_cv_clip\"\n",
        "IMAGES_DIR=\"${ROOT}/images\"      # 다이어그램 원본\n",
        "ICONS_DIR=\"${ROOT}/icons\"        # AWS 아이콘 템플릿 (카테고리별 PNG들)\n",
        "OUT_DIR=\"${ROOT}/out\"\n",
        "TAXONOMY_CSV=\"/mnt/data/aws_resources_models.csv\"\n",
        "mkdir -p \"$ROOT\"/{src,$IMAGES_DIR,$ICONS_DIR,$OUT_DIR}\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ============ Config ============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${ROOT}/config.yaml\" <<'YAML'\n",
        "data:\n",
        "  images_dir: ./images\n",
        "  icons_dir: ./icons           # 각 아이콘 PNG (가능하면 흰 배경/정중앙)\n",
        "  taxonomy_csv: /mnt/data/aws_resources_models.csv\n",
        "model:\n",
        "  clip_name: ViT-B-32           # open_clip 모델명 (경량/빠름)\n",
        "  clip_pretrained: laion2b_s34b_b79k\n",
        "detect:\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # 후보 탐색 파라미터 (과하면 느림 → 정확도↑ / 줄이면 빠름)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "  max_size: 1600         # 긴 변 리사이즈 상한\n",
        "  canny_low: 60\n",
        "  canny_high: 160\n",
        "  mser_delta: 5\n",
        "  min_area: 900\n",
        "  max_area: 90000\n",
        "  win: 128               # 슬라이딩 윈도우 크기\n",
        "  stride: 96\n",
        "  iou_nms: 0.45\n",
        "retrieval:\n",
        "  topk: 5                # CLIP k-NN 후보 수\n",
        "  orb_refine: true       # ORB 정합 재채점\n",
        "  orb_nfeatures: 500\n",
        "  score_clip_w: 0.7      # 가중합: clip 0.7 + orb 0.3 + ocr 0.1\n",
        "  score_orb_w: 0.3\n",
        "  score_ocr_w: 0.1\n",
        "  accept_score: 0.35     # 최종 수락 임계값\n",
        "ocr:\n",
        "  enabled: true          # easyocr 설치되면 자동 켜짐\n",
        "  lang: [\"en\"]           # \"S3\", \"EC2\", \"RDS\" 등 약어 픽업 용\n",
        "output:\n",
        "  dir: ./out\n",
        "  format: labelstudio    # yolo | coco | labelstudio\n",
        "YAML\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ============ src: Taxonomy ============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${ROOT}/src/taxonomy.py\" <<'PY'\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "from rapidfuzz import process, fuzz\n",
        "@dataclass\n",
        "class Taxonomy:\n",
        "    canonical_to_aliases: Dict[str, List[str]]\n",
        "    alias_to_canonical: Dict[str, str]\n",
        "    names: List[str]\n",
        "    @classmethod\n",
        "    def from_csv(cls, path: str) -> \"Taxonomy\":\n",
        "        df = pd.read_csv(path)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # 열 추론\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "        def pick(cols, cands):\n",
        "            for c in cols:\n",
        "                if c.lower() in cands: return c\n",
        "            return cols[0]\n",
        "        name_col = pick(df.columns, {\"canonical\",\"name\",\"service\",\"label\"})\n",
        "        alias_col = None\n",
        "        for c in df.columns:\n",
        "            if c.lower() in {\"aliases\",\"alias\",\"aka\"}: alias_col = c; break\n",
        "        c2a, a2c = {}, {}\n",
        "        for _, r in df.iterrows():\n",
        "            canon = str(r[name_col]).strip()\n",
        "            aliases = []\n",
        "            if alias_col and pd.notna(r[alias_col]):\n",
        "                aliases = [a.strip() for a in str(r[alias_col]).split(\"|\") if a.strip()]\n",
        "            keys = set([canon] + aliases)\n",
        "            c2a[canon] = list(keys)\n",
        "            for k in keys:\n",
        "                a2c[k.lower()] = canon\n",
        "        return cls(c2a, a2c, list(c2a.keys()))\n",
        "    def normalize(self, s: str) -> Tuple[str,float]:\n",
        "        key = s.strip().lower()\n",
        "        if key in self.alias_to_canonical: return self.alias_to_canonical[key], 1.0\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # alias fuzzy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "        best = process.extractOne(key, list(self.alias_to_canonical.keys()), scorer=fuzz.WRatio)\n",
        "        if best:\n",
        "            alias, sc, _ = best\n",
        "            return self.alias_to_canonical[alias], sc/100.0\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # fallback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "        best2 = process.extractOne(key, self.names, scorer=fuzz.WRatio)\n",
        "        if best2:\n",
        "            nm, sc, _ = best2\n",
        "            return nm, sc/100.0\n",
        "        return s, 0.0\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ============ src: Proposals (CV 후보 탐지) ============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${ROOT}/src/proposals.py\" <<'PY'\n",
        "import cv2, numpy as np\n",
        "def preprocess_resize(img, max_size=1600):\n",
        "    h, w = img.shape[:2]\n",
        "    s = max(h, w)\n",
        "    if s <= max_size: return img, 1.0\n",
        "    r = max_size / s\n",
        "    img2 = cv2.resize(img, (int(w*r), int(h*r)), interpolation=cv2.INTER_AREA)\n",
        "    return img2, r\n",
        "def edges_and_mser(img, canny_low=60, canny_high=160, mser_delta=5, min_area=900, max_area=90000):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    e = cv2.Canny(gray, canny_low, canny_high)\n",
        "    cnts, _ = cv2.findContours(e, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    boxes = []\n",
        "    for c in cnts:\n",
        "        x,y,w,h = cv2.boundingRect(c)\n",
        "        a = w*h\n",
        "        if min_area <= a <= max_area:\n",
        "            boxes.append((x,y,w,h))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # MSER (문자/아이콘 내부 강한 blob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    mser = cv2.MSER_create(_delta=mser_delta)\n",
        "    regions, _ = mser.detectRegions(gray)\n",
        "    for r in regions:\n",
        "        x,y,w,h = cv2.boundingRect(r.reshape(-1,1,2))\n",
        "        a = w*h\n",
        "        if min_area <= a <= max_area:\n",
        "            boxes.append((x,y,w,h))\n",
        "    return boxes\n",
        "def sliding_windows(img, win=128, stride=96):\n",
        "    H,W = img.shape[:2]\n",
        "    for y in range(0, max(1, H-win), stride):\n",
        "        for x in range(0, max(1, W-win), stride):\n",
        "            yield (x,y,win,win)\n",
        "def propose(img_bgr, cfg):\n",
        "    img, r = preprocess_resize(img_bgr, cfg[\"max_size\"])\n",
        "    boxes = []\n",
        "    boxes += edges_and_mser(img, cfg[\"canny_low\"], cfg[\"canny_high\"], cfg[\"mser_delta\"], cfg[\"min_area\"], cfg[\"max_area\"])\n",
        "    boxes += list(sliding_windows(img, cfg[\"win\"], cfg[\"stride\"]))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # 스케일 복원\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    if r != 1.0:\n",
        "        boxes = [(int(x/r),int(y/r),int(w/r),int(h/r)) for (x,y,w,h) in boxes]\n",
        "    return boxes\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ============ src: CLIP Index ============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${ROOT}/src/clip_index.py\" <<'PY'\n",
        "import os, cv2, torch, faiss, numpy as np\n",
        "import open_clip\n",
        "from typing import List, Tuple\n",
        "def load_clip(name, pretrained, device):\n",
        "    model, preprocess, _ = open_clip.create_model_and_transforms(name, pretrained=pretrained, device=device)\n",
        "    model.eval()\n",
        "    return model, preprocess\n",
        "def img_to_feat(model, preprocess, pil, device):\n",
        "    with torch.no_grad():\n",
        "        im = preprocess(pil).unsqueeze(0).to(device)\n",
        "        f = model.encode_image(im)\n",
        "        f = f / f.norm(dim=-1, keepdim=True)\n",
        "    return f.squeeze(0).cpu().numpy()\n",
        "def build_icon_index(icons_dir, model, preprocess, device):\n",
        "    paths, feats = [], []\n",
        "    for root, _, fs in os.walk(icons_dir):\n",
        "        for f in fs:\n",
        "            if f.lower().endswith((\".png\",\".jpg\",\".jpeg\",\".webp\")):\n",
        "                p = os.path.join(root,f)\n",
        "                paths.append(p)\n",
        "    from PIL import Image\n",
        "    for p in paths:\n",
        "        pil = Image.open(p).convert(\"RGB\")\n",
        "        feats.append(img_to_feat(model, preprocess, pil, device))\n",
        "    feats = np.stack(feats).astype(\"float32\")\n",
        "    idx = faiss.IndexFlatIP(feats.shape[1])\n",
        "    idx.add(feats)\n",
        "    return paths, feats, idx\n",
        "def search(index, feats_matrix, query_feat, topk=5):\n",
        "    D, I = index.search(query_feat[None,:].astype(\"float32\"), topk)\n",
        "    return D[0], I[0]\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ============ src: ORB Template Refinement ============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${ROOT}/src/orb_refine.py\" <<'PY'\n",
        "import cv2, numpy as np\n",
        "def orb_score(patch_bgr, icon_bgr, nfeatures=500):\n",
        "    orb = cv2.ORB_create(nfeatures=nfeatures)\n",
        "    kp1, des1 = orb.detectAndCompute(cv2.cvtColor(patch_bgr, cv2.COLOR_BGR2GRAY), None)\n",
        "    kp2, des2 = orb.detectAndCompute(cv2.cvtColor(icon_bgr, cv2.COLOR_BGR2GRAY), None)\n",
        "    if des1 is None or des2 is None or len(kp1)<5 or len(kp2)<5: return 0.0\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    m = bf.match(des1, des2)\n",
        "    if not m: return 0.0\n",
        "    m = sorted(m, key=lambda x: x.distance)\n",
        "    good = [x for x in m if x.distance < 64]   # distance 가 작을수록 유사\n",
        "    return min(1.0, len(good)/max(10, len(m)))\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ============ src: OCR (optional) ============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${ROOT}/src/ocr_hint.py\" <<'PY'\n",
        "def ocr_text(pil, lang=(\"en\",)):\n",
        "    try:\n",
        "        import easyocr\n",
        "        r = easyocr.Reader(list(lang), gpu=False)\n",
        "        res = r.readtext(np.array(pil))\n",
        "        txt = \" \".join([t[1] for t in res]) if res else \"\"\n",
        "        return txt\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "import numpy as np\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ============ src: Exporters ============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${ROOT}/src/exporters.py\" <<'PY'\n",
        "import os, json\n",
        "def to_labelstudio(items):\n",
        "    out = []\n",
        "    for it in items:\n",
        "        W,H = it[\"width\"], it[\"height\"]\n",
        "        ann = []\n",
        "        for o in it[\"objects\"]:\n",
        "            x,y,w,h = o[\"bbox\"]\n",
        "            ann.append({\n",
        "                \"from_name\":\"label\",\"to_name\":\"image\",\"type\":\"rectanglelabels\",\n",
        "                \"value\":{\n",
        "                    \"x\": x/W*100, \"y\": y/H*100, \"width\": w/W*100, \"height\": h/H*100,\n",
        "                    \"rectanglelabels\":[o[\"label\"]], \"score\": o.get(\"score\",None)\n",
        "                }\n",
        "            })\n",
        "        out.append({\"data\":{\"image\": os.path.basename(it[\"image_path\"])}, \"annotations\":[{\"result\":ann}]})\n",
        "    return out\n",
        "def to_yolo(items, out_dir):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    name2id, next_id = {}, 0\n",
        "    for it in items:\n",
        "        W,H = it[\"width\"], it[\"height\"]\n",
        "        stem = os.path.splitext(os.path.basename(it[\"image_path\"]))[0]\n",
        "        lines=[]\n",
        "        for o in it[\"objects\"]:\n",
        "            x,y,w,h=o[\"bbox\"]\n",
        "            cx,cy=(x+w/2)/W,(y+h/2)/H\n",
        "            ww,hh=w/W,h/H\n",
        "            name=o[\"label\"]\n",
        "            if name not in name2id:\n",
        "                name2id[name]=next_id; next_id+=1\n",
        "            cid=name2id[name]\n",
        "            lines.append(f\"{cid} {cx:.6f} {cy:.6f} {ww:.6f} {hh:.6f}\")\n",
        "        with open(os.path.join(out_dir, f\"{stem}.txt\"),\"w\") as f:\n",
        "            f.write(\"\\n\".join(lines))\n",
        "    with open(os.path.join(out_dir,\"classes.txt\"),\"w\") as f:\n",
        "        for n,_id in sorted(name2id.items(), key=lambda x:x[1]):\n",
        "            f.write(n+\"\\n\")\n",
        "PY\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ============ src: Main ============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cat > \"${ROOT}/src/run.py\" <<'PY'\n",
        "import os, yaml, json, cv2, numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from taxonomy import Taxonomy\n",
        "from proposals import propose\n",
        "from clip_index import load_clip, build_icon_index, img_to_feat, search\n",
        "from exporters import to_labelstudio, to_yolo\n",
        "from orb_refine import orb_score\n",
        "from ocr_hint import ocr_text\n",
        "def nms(boxes, scores, iou_thr=0.45):\n",
        "    import numpy as np\n",
        "    boxes = np.array(boxes, dtype=float)\n",
        "    scores = np.array(scores, dtype=float)\n",
        "    x1=boxes[:,0]; y1=boxes[:,1]; x2=boxes[:,0]+boxes[:,2]; y2=boxes[:,1]+boxes[:,3]\n",
        "    areas=(x2-x1+1)*(y2-y1+1); order=scores.argsort()[::-1]\n",
        "    keep=[]\n",
        "    while order.size>0:\n",
        "        i=order[0]; keep.append(i)\n",
        "        xx1=np.maximum(x1[i],x1[order[1:]])\n",
        "        yy1=np.maximum(y1[i],y1[order[1:]])\n",
        "        xx2=np.minimum(x2[i],x2[order[1:]])\n",
        "        yy2=np.minimum(y2[i],y2[order[1:]])\n",
        "        w=np.maximum(0.0,xx2-xx1+1); h=np.maximum(0.0,yy2-yy1+1)\n",
        "        inter=w*h; iou=inter/(areas[i]+areas[order[1:]]-inter+1e-6)\n",
        "        inds=np.where(iou<=iou_thr)[0]; order=order[inds+1]\n",
        "    return keep\n",
        "def load_cfg():\n",
        "    with open(\"config.yaml\",\"r\") as f:\n",
        "        return yaml.safe_load(f)\n",
        "def list_images(d):\n",
        "    out=[]\n",
        "    for r,_,fs in os.walk(d):\n",
        "        for f in fs:\n",
        "            if f.lower().endswith((\".png\",\".jpg\",\".jpeg\",\".webp\")):\n",
        "                out.append(os.path.join(r,f))\n",
        "    return sorted(out)\n",
        "def main():\n",
        "    cfg = load_cfg()\n",
        "    os.makedirs(cfg[\"output\"][\"dir\"], exist_ok=True)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # Taxonomy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    tax = Taxonomy.from_csv(cfg[\"data\"][\"taxonomy_csv\"])\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # CLIP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    import torch\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model, preprocess = load_clip(cfg[\"model\"][\"clip_name\"], cfg[\"model\"][\"clip_pretrained\"], device)\n",
        "    icon_paths, icon_feats, index = build_icon_index(cfg[\"data\"][\"icons_dir\"], model, preprocess, device)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    ims = list_images(cfg[\"data\"][\"images_dir\"])\n",
        "    results=[]\n",
        "    for p in tqdm(ims, desc=\"AutoLabel (CV+CLIP)\"):\n",
        "        img_bgr = cv2.imread(p, cv2.IMREAD_COLOR); H,W=img_bgr.shape[:2]\n",
        "        boxes = propose(img_bgr, cfg[\"detect\"])\n",
        "        objects=[]\n",
        "        for (x,y,w,h) in boxes:\n",
        "            x0,y0,x1,y1 = max(0,x),max(0,y),min(W,x+w),min(H,y+h)\n",
        "            if x1-x0<24 or y1-y0<24: continue\n",
        "            crop = Image.fromarray(cv2.cvtColor(img_bgr[y0:y1, x0:x1], cv2.COLOR_BGR2RGB))\n",
        "            qf = img_to_feat(model, preprocess, crop, device)\n",
        "            D, I = index.search(icon_feats, qf, topk=cfg[\"retrieval\"][\"topk\"]) if False else (None, None)  # placeholder\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # 위 라인 수정: search() 시그니처 (index, feats, query_feat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "            from clip_index import search as faiss_search\n",
        "            D, I = faiss_search(index, icon_feats, qf, cfg[\"retrieval\"][\"topk\"])\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # clip sim ∈ [-1,1] → [0,1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "            clip_scores = [(icon_paths[i], float((d+1)/2)) for d,i in zip(D,I)]\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # ORB refine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "            orb_s=0.0\n",
        "            if cfg[\"retrieval\"][\"orb_refine\"]:\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # 상위1 템플릿로만 정합\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "                best_icon = cv2.imread(clip_scores[0][0], cv2.IMREAD_COLOR)\n",
        "                orb_s = orb_score(cv2.cvtColor(np.array(crop), cv2.COLOR_RGB2BGR), best_icon, nfeatures=cfg[\"retrieval\"][\"orb_nfeatures\"])\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # OCR hint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "            ocr_s = 0.0\n",
        "            if cfg[\"ocr\"][\"enabled\"]:\n",
        "                txt = ocr_text(crop, tuple(cfg[\"ocr\"][\"lang\"]))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # 간단 가중: 텍스트가 있으면 약한 가산점\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "                ocr_s = 0.2 if (txt and len(txt) <= 12) else 0.0\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # 가중합\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "            s_clip = clip_scores[0][1]\n",
        "            s = cfg[\"retrieval\"][\"score_clip_w\"]*s_clip + cfg[\"retrieval\"][\"score_orb_w\"]*orb_s + cfg[\"retrieval\"][\"score_ocr_w\"]*ocr_s\n",
        "            if s < cfg[\"retrieval\"][\"accept_score\"]:\n",
        "                continue\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # 라벨명: 템플릿 파일명 → taxonomy normalize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "            label_raw = os.path.splitext(os.path.basename(clip_scores[0][0]))[0]\n",
        "            label, nsc = tax.normalize(label_raw)\n",
        "            objects.append({\"bbox\":[x0,y0,x1-x0,y1-y0], \"label\": label, \"score\": round(s*0.7 + nsc*0.3, 4)})\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # NMS + 정렬\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "        if objects:\n",
        "            keep = nms([o[\"bbox\"] for o in objects], [o[\"score\"] for o in objects], cfg[\"detect\"][\"iou_nms\"])\n",
        "            objects = [objects[i] for i in keep]\n",
        "            objects = sorted(objects, key=lambda o: -o[\"score\"])\n",
        "        results.append({\"image_path\": p, \"width\": W, \"height\": H, \"objects\": objects})\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## # Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "    if cfg[\"output\"][\"format\"] == \"yolo\":\n",
        "        to_yolo(results, os.path.join(cfg[\"output\"][\"dir\"], \"yolo\"))\n",
        "    else:\n",
        "        js = to_labelstudio(results)\n",
        "        with open(os.path.join(cfg[\"output\"][\"dir\"], \"labelstudio.json\"),\"w\",encoding=\"utf-8\") as f:\n",
        "            json.dump(js, f, ensure_ascii=False, indent=2)\n",
        "    with open(os.path.join(cfg[\"output\"][\"dir\"], \"raw_items.json\"),\"w\",encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n",
        "cat <<'EOM'\n",
        "[사용법]\n",
        "1) ./aws_cv_clip/icons/ 에 AWS 아이콘 PNG를 넣으세요.\n",
        "   - 파일명 = 정규화 전 라벨 원형 (예: \"Amazon-S3.png\", \"AWS-Lambda.png\")\n",
        "   - 여러 버전이 있으면 서브폴더 포함 가능 (폴더명은 무시, 파일명 기준)\n",
        "2) ./aws_cv_clip/images/ 에 다이어그램 이미지를 넣으세요.\n",
        "3) 실행:\n",
        "   cd aws_cv_clip\n",
        "   source .venv/bin/activate\n",
        "   python src/run.py\n",
        "4) 결과:\n",
        "   - out/labelstudio.json (Label Studio 임포트)\n",
        "   - out/raw_items.json (중간 산출)\n",
        "   - out/yolo/* (OUT_FORMAT=yolo로 바꾸면)\n",
        "[팁]\n",
        "- 속도↑: config.yaml → detect.win=160, stride=128, min_area 상향\n",
        "- 정확도↑: icons 템플릿을 \"정면/단색\" 버전으로 통일, 배경 투명 PNG 권장\n",
        "- GPU가 있으면 open_clip_torch가 자동 CUDA 사용 (속도↑)\n",
        "EOM"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}