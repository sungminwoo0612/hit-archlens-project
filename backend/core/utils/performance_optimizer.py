"""
Performance Optimizer

ê·¹ì ì¸ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ìµœì í™” ë„êµ¬ë“¤
"""

import time
import psutil
import torch
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
import json
import threading
from dataclasses import dataclass
from contextlib import contextmanager
import gc
import tracemalloc


@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    processing_time: float
    memory_usage: float
    cpu_usage: float
    gpu_usage: Optional[float] = None
    throughput: float = 0.0
    latency: float = 0.0
    cache_hit_rate: float = 0.0
    batch_efficiency: float = 0.0


class PerformanceOptimizer:
    """
    ì„±ëŠ¥ ìµœì í™” ë„êµ¬
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    2. ë©”ëª¨ë¦¬ ìµœì í™”
    3. GPU ìµœì í™”
    4. ìºì‹œ ìµœì í™”
    5. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        ì„±ëŠ¥ ìµœì í™” ë„êµ¬ ì´ˆê¸°í™”
        
        Args:
            config: ì„±ëŠ¥ ì„¤ì •
        """
        self.config = config
        self.metrics_history: List[PerformanceMetrics] = []
        self.optimization_enabled = config.get("performance", {}).get("enable_optimization", True)
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
        self.memory_tracker = MemoryTracker()
        
        # GPU ëª¨ë‹ˆí„°ë§
        self.gpu_tracker = GPUTracker() if torch.cuda.is_available() else None
        
        # ìºì‹œ ìµœì í™”
        self.cache_optimizer = CacheOptimizer(config)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
        self.batch_optimizer = BatchOptimizer(config)
        
        print("ğŸš€ Performance Optimizer ì´ˆê¸°í™” ì™„ë£Œ")
    
    @contextmanager
    def performance_monitoring(self, operation_name: str):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        if not self.optimization_enabled:
            yield
            return
        
        start_time = time.time()
        start_memory = self.memory_tracker.get_memory_usage()
        start_cpu = psutil.cpu_percent()
        start_gpu = self.gpu_tracker.get_gpu_usage() if self.gpu_tracker else None
        
        try:
            yield
        finally:
            end_time = time.time()
            end_memory = self.memory_tracker.get_memory_usage()
            end_cpu = psutil.cpu_percent()
            end_gpu = self.gpu_tracker.get_gpu_usage() if self.gpu_tracker else None
            
            metrics = PerformanceMetrics(
                processing_time=end_time - start_time,
                memory_usage=end_memory - start_memory,
                cpu_usage=(start_cpu + end_cpu) / 2,
                gpu_usage=(start_gpu + end_gpu) / 2 if start_gpu and end_gpu else None
            )
            
            self.metrics_history.append(metrics)
            self._log_performance(operation_name, metrics)
    
    def optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        if not self.optimization_enabled:
            return
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        # PyTorch ìºì‹œ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        memory_usage = self.memory_tracker.get_memory_usage()
        if memory_usage > 0.8:  # 80% ì´ìƒ ì‚¬ìš© ì‹œ
            print(f"âš ï¸ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì§€: {memory_usage:.1%}")
            self._emergency_memory_cleanup()
    
    def optimize_gpu(self):
        """GPU ìµœì í™”"""
        if not self.gpu_tracker or not self.optimization_enabled:
            return
        
        gpu_usage = self.gpu_tracker.get_gpu_usage()
        if gpu_usage and gpu_usage > 0.9:  # 90% ì´ìƒ ì‚¬ìš© ì‹œ
            print(f"âš ï¸ ë†’ì€ GPU ì‚¬ìš©ëŸ‰ ê°ì§€: {gpu_usage:.1%}")
            torch.cuda.empty_cache()
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.metrics_history:
            return {"error": "No metrics available"}
        
        recent_metrics = self.metrics_history[-100:]  # ìµœê·¼ 100ê°œ
        
        avg_processing_time = np.mean([m.processing_time for m in recent_metrics])
        avg_memory_usage = np.mean([m.memory_usage for m in recent_metrics])
        avg_cpu_usage = np.mean([m.cpu_usage for m in recent_metrics])
        
        report = {
            "summary": {
                "total_operations": len(self.metrics_history),
                "avg_processing_time": avg_processing_time,
                "avg_memory_usage": avg_memory_usage,
                "avg_cpu_usage": avg_cpu_usage,
                "optimization_enabled": self.optimization_enabled
            },
            "recent_performance": {
                "last_10_operations": [
                    {
                        "processing_time": m.processing_time,
                        "memory_usage": m.memory_usage,
                        "cpu_usage": m.cpu_usage
                    }
                    for m in recent_metrics[-10:]
                ]
            },
            "recommendations": self._generate_recommendations()
        }
        
        return report
    
    def _log_performance(self, operation_name: str, metrics: PerformanceMetrics):
        """ì„±ëŠ¥ ë¡œê¹…"""
        print(f"ğŸ“Š {operation_name}: {metrics.processing_time:.3f}s, "
              f"Memory: {metrics.memory_usage:.1%}, CPU: {metrics.cpu_usage:.1%}")
    
    def _emergency_memory_cleanup(self):
        """ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        print("ğŸš¨ ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰")
        
        # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        # PyTorch ìºì‹œ ì™„ì „ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¬í™•ì¸
        memory_usage = self.memory_tracker.get_memory_usage()
        print(f"âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {memory_usage:.1%}")
    
    def _generate_recommendations(self) -> List[str]:
        """ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        recent_metrics = self.metrics_history[-50:]
        avg_processing_time = np.mean([m.processing_time for m in recent_metrics])
        avg_memory_usage = np.mean([m.memory_usage for m in recent_metrics])
        
        if avg_processing_time > 1.0:
            recommendations.append("ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë ¤ ì²˜ë¦¬ ì‹œê°„ì„ ë‹¨ì¶•í•˜ì„¸ìš”")
        
        if avg_memory_usage > 0.7:
            recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ëª¨ë¸ ì–‘ìí™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")
        
        if len(recommendations) == 0:
            recommendations.append("í˜„ì¬ ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤")
        
        return recommendations


class MemoryTracker:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì ê¸°"""
    
    def __init__(self):
        self.process = psutil.Process()
    
    def get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (0.0 ~ 1.0)"""
        memory_info = self.process.memory_info()
        return memory_info.rss / psutil.virtual_memory().total
    
    def get_memory_info(self) -> Dict[str, Any]:
        """ìƒì„¸ ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
        memory_info = self.process.memory_info()
        virtual_memory = psutil.virtual_memory()
        
        return {
            "rss": memory_info.rss,
            "vms": memory_info.vms,
            "percent": memory_info.rss / virtual_memory.total,
            "available": virtual_memory.available,
            "total": virtual_memory.total
        }


class GPUTracker:
    """GPU ì‚¬ìš©ëŸ‰ ì¶”ì ê¸°"""
    
    def __init__(self):
        self.device = torch.cuda.current_device() if torch.cuda.is_available() else None
    
    def get_gpu_usage(self) -> Optional[float]:
        """GPU ì‚¬ìš©ëŸ‰ ë°˜í™˜ (0.0 ~ 1.0)"""
        if not torch.cuda.is_available():
            return None
        
        try:
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            allocated = torch.cuda.memory_allocated(self.device)
            reserved = torch.cuda.memory_reserved(self.device)
            total = torch.cuda.get_device_properties(self.device).total_memory
            
            return allocated / total
        except:
            return None
    
    def get_gpu_info(self) -> Dict[str, Any]:
        """GPU ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        if not torch.cuda.is_available():
            return {"error": "GPU not available"}
        
        try:
            props = torch.cuda.get_device_properties(self.device)
            allocated = torch.cuda.memory_allocated(self.device)
            reserved = torch.cuda.memory_reserved(self.device)
            
            return {
                "name": props.name,
                "total_memory": props.total_memory,
                "allocated_memory": allocated,
                "reserved_memory": reserved,
                "memory_usage": allocated / props.total_memory
            }
        except:
            return {"error": "Failed to get GPU info"}


class CacheOptimizer:
    """ìºì‹œ ìµœì í™” ë„êµ¬"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.cache_config = config.get("caching", {})
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "size": 0
        }
    
    def get_cache_hit_rate(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨ ë°˜í™˜"""
        total = self.cache_stats["hits"] + self.cache_stats["misses"]
        return self.cache_stats["hits"] / total if total > 0 else 0.0
    
    def optimize_cache_size(self, target_hit_rate: float = 0.8):
        """ìºì‹œ í¬ê¸° ìµœì í™”"""
        current_hit_rate = self.get_cache_hit_rate()
        
        if current_hit_rate < target_hit_rate:
            # ìºì‹œ í¬ê¸° ì¦ê°€
            new_size = int(self.cache_stats["size"] * 1.5)
            print(f"ğŸ“ˆ ìºì‹œ í¬ê¸° ì¦ê°€: {self.cache_stats['size']} -> {new_size}")
            self.cache_stats["size"] = new_size
        elif current_hit_rate > 0.95:
            # ìºì‹œ í¬ê¸° ê°ì†Œ
            new_size = int(self.cache_stats["size"] * 0.8)
            print(f"ğŸ“‰ ìºì‹œ í¬ê¸° ê°ì†Œ: {self.cache_stats['size']} -> {new_size}")
            self.cache_stats["size"] = new_size


class BatchOptimizer:
    """ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ë„êµ¬"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.performance_config = config.get("performance", {})
        self.current_batch_size = self.performance_config.get("batch_size", 16)
        self.batch_history = []
    
    def optimize_batch_size(self, processing_times: List[float]) -> int:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        if len(processing_times) < 5:
            return self.current_batch_size
        
        # ì²˜ë¦¬ ì‹œê°„ê³¼ ë°°ì¹˜ í¬ê¸°ì˜ ê´€ê³„ ë¶„ì„
        avg_time = np.mean(processing_times)
        
        if avg_time < 0.1:  # ë§¤ìš° ë¹ ë¦„
            new_batch_size = int(self.current_batch_size * 1.5)
        elif avg_time > 1.0:  # ë§¤ìš° ëŠë¦¼
            new_batch_size = int(self.current_batch_size * 0.7)
        else:
            new_batch_size = self.current_batch_size
        
        # ë²”ìœ„ ì œí•œ
        new_batch_size = max(1, min(new_batch_size, 128))
        
        if new_batch_size != self.current_batch_size:
            print(f"ğŸ”„ ë°°ì¹˜ í¬ê¸° ìµœì í™”: {self.current_batch_size} -> {new_batch_size}")
            self.current_batch_size = new_batch_size
        
        return self.current_batch_size
    
    def get_optimal_batch_size(self) -> int:
        """í˜„ì¬ ìµœì  ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
        return self.current_batch_size


# ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ë°ì½”ë ˆì´í„°
def profile_performance(func):
    """í•¨ìˆ˜ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ë°ì½”ë ˆì´í„°"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss
        
        result = func(*args, **kwargs)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss
        
        processing_time = end_time - start_time
        memory_usage = (end_memory - start_memory) / 1024 / 1024  # MB
        
        print(f"ğŸ“Š {func.__name__}: {processing_time:.3f}s, Memory: {memory_usage:.1f}MB")
        
        return result
    
    return wrapper


# ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
@contextmanager
def memory_profiling():
    """ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    tracemalloc.start()
    start_snapshot = tracemalloc.take_snapshot()
    
    try:
        yield
    finally:
        end_snapshot = tracemalloc.take_snapshot()
        tracemalloc.stop()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
        top_stats = end_snapshot.compare_to(start_snapshot, 'lineno')
        
        print("ğŸ“Š ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ê²°ê³¼:")
        for stat in top_stats[:5]:
            print(f"  {stat.count_diff:+d} blocks: {stat.size_diff/1024:.1f}KB")
            print(f"    {stat.traceback.format()}")
