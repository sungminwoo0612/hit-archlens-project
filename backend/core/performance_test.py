#!/usr/bin/env python3
"""
Performance Test Script

ê·¹ì ì¸ ì„±ëŠ¥ ê°œì„  íš¨ê³¼ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import time
import json
import yaml
from pathlib import Path
from typing import Dict, Any, List
import torch
import numpy as np
from PIL import Image

from .auto_labeler.ultra_fast_cv_labeler import UltraFastCVAutoLabeler
from .utils.performance_optimizer import PerformanceOptimizer, profile_performance
from .models import CloudProvider


class PerformanceTester:
    """
    ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë„êµ¬
    
    ì£¼ìš” í…ŒìŠ¤íŠ¸:
    1. ì²˜ë¦¬ ì†ë„ ë¹„êµ
    2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
    3. ì •í™•ë„ ë¹„êµ
    4. ìŠ¤ì¼€ì¼ë§ í…ŒìŠ¤íŠ¸
    """
    
    def __init__(self, config_path: str = "backend/configs/ultra_performance_config.yaml"):
        """
        ì„±ëŠ¥ í…ŒìŠ¤í„° ì´ˆê¸°í™”
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = self._load_config(config_path)
        self.performance_optimizer = PerformanceOptimizer(self.config)
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¤€ë¹„
        self.test_images = self._load_test_images()
        
        print("ğŸ§ª Performance Tester ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _load_test_images(self) -> List[Image.Image]:
        """í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ"""
        images = []
        
        # ì‹¤ì œ AWS ì•„ì´ì½˜ ì´ë¯¸ì§€ ì‚¬ìš©
        icon_dir = Path("aws_data_collectors/collectors/collected_icons/raw")
        if icon_dir.exists():
            icon_files = list(icon_dir.glob("*.png")) + list(icon_dir.glob("*.jpg"))
            for icon_file in icon_files[:10]:  # ìƒìœ„ 10ê°œë§Œ ì‚¬ìš©
                try:
                    image = Image.open(icon_file)
                    images.append(image)
                    print(f"ğŸ“ ë¡œë“œëœ ì´ë¯¸ì§€: {icon_file.name}")
                except Exception as e:
                    print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {icon_file.name} - {e}")
        
        # ì‹¤ì œ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        if not images:
            print("ğŸ“ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
            for i in range(10):
                # AWS ì„œë¹„ìŠ¤ ìƒ‰ìƒì„ ë°˜ì˜í•œ ë”ë¯¸ ì´ë¯¸ì§€
                img = Image.new('RGB', (256, 256), color=(255, 153, 0))  # AWS ì˜¤ë Œì§€
                images.append(img)
        
        return images
    
    @profile_performance
    def test_processing_speed(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸"""
        print("ğŸƒâ€â™‚ï¸ ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        results = {
            "total_images": len(self.test_images),
            "processing_times": [],
            "avg_processing_time": 0.0,
            "throughput": 0.0
        }
        
        # Ultra Fast CV Labeler ì´ˆê¸°í™”
        labeler = UltraFastCVAutoLabeler(CloudProvider.AWS, self.config)
        
        # ê° ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
        for i, image in enumerate(self.test_images):
            with self.performance_optimizer.performance_monitoring(f"Image_{i}"):
                start_time = time.time()
                
                detections = labeler._analyze_single_image(image)
                
                processing_time = time.time() - start_time
                results["processing_times"].append(processing_time)
                
                print(f"  ì´ë¯¸ì§€ {i+1}: {processing_time:.3f}ì´ˆ, {len(detections)}ê°œ ê°ì§€")
        
        # í†µê³„ ê³„ì‚°
        results["avg_processing_time"] = np.mean(results["processing_times"])
        results["throughput"] = len(self.test_images) / sum(results["processing_times"])
        
        print(f"ğŸ“Š ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {results['avg_processing_time']:.3f}ì´ˆ")
        print(f"  ì²˜ë¦¬ëŸ‰: {results['throughput']:.2f} ì´ë¯¸ì§€/ì´ˆ")
        
        return results
    
    @profile_performance
    def test_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        print("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        results = {
            "memory_usage": [],
            "peak_memory": 0.0,
            "avg_memory": 0.0
        }
        
        # Ultra Fast CV Labeler ì´ˆê¸°í™”
        labeler = UltraFastCVAutoLabeler(CloudProvider.AWS, self.config)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        for i, image in enumerate(self.test_images):
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            memory_before = self.performance_optimizer.memory_tracker.get_memory_usage()
            
            detections = labeler._analyze_single_image(image)
            
            memory_after = self.performance_optimizer.memory_tracker.get_memory_usage()
            memory_used = memory_after - memory_before
            
            results["memory_usage"].append(memory_used)
            
            print(f"  ì´ë¯¸ì§€ {i+1}: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ {memory_used:.1%}")
        
        # í†µê³„ ê³„ì‚°
        results["peak_memory"] = max(results["memory_usage"])
        results["avg_memory"] = np.mean(results["memory_usage"])
        
        print(f"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"  í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {results['avg_memory']:.1%}")
        print(f"  ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {results['peak_memory']:.1%}")
        
        return results
    
    @profile_performance
    def test_scaling_performance(self) -> Dict[str, Any]:
        """ìŠ¤ì¼€ì¼ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ“ˆ ìŠ¤ì¼€ì¼ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        results = {
            "batch_sizes": [1, 2, 4, 8, 16],
            "processing_times": [],
            "throughput": [],
            "memory_usage": []
        }
        
        # Ultra Fast CV Labeler ì´ˆê¸°í™”
        labeler = UltraFastCVAutoLabeler(CloudProvider.AWS, self.config)
        
        # ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
        for batch_size in results["batch_sizes"]:
            print(f"  ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ë°°ì¹˜ í¬ê¸° ì„¤ì •
            labeler.batch_size = batch_size
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            memory_before = self.performance_optimizer.memory_tracker.get_memory_usage()
            
            # ë°°ì¹˜ë¡œ ì´ë¯¸ì§€ ì²˜ë¦¬
            for i in range(0, len(self.test_images), batch_size):
                batch_images = self.test_images[i:i+batch_size]
                for image in batch_images:
                    detections = labeler._analyze_single_image(image)
            
            processing_time = time.time() - start_time
            memory_after = self.performance_optimizer.memory_tracker.get_memory_usage()
            
            results["processing_times"].append(processing_time)
            results["throughput"].append(len(self.test_images) / processing_time)
            results["memory_usage"].append(memory_after - memory_before)
            
            print(f"    ì²˜ë¦¬ ì‹œê°„: {processing_time:.3f}ì´ˆ")
            print(f"    ì²˜ë¦¬ëŸ‰: {results['throughput'][-1]:.2f} ì´ë¯¸ì§€/ì´ˆ")
            print(f"    ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {results['memory_usage'][-1]:.1%}")
        
        print(f"ğŸ“Š ìŠ¤ì¼€ì¼ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        for i, batch_size in enumerate(results["batch_sizes"]):
            print(f"  ë°°ì¹˜ í¬ê¸° {batch_size}: {results['throughput'][i]:.2f} ì´ë¯¸ì§€/ì´ˆ")
        
        return results
    
    @profile_performance
    def test_accuracy_comparison(self) -> Dict[str, Any]:
        """ì •í™•ë„ ë¹„êµ í…ŒìŠ¤íŠ¸"""
        print("ğŸ¯ ì •í™•ë„ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        results = {
            "detection_counts": [],
            "confidence_scores": [],
            "avg_confidence": 0.0
        }
        
        # Ultra Fast CV Labeler ì´ˆê¸°í™”
        labeler = UltraFastCVAutoLabeler(CloudProvider.AWS, self.config)
        
        # ê° ì´ë¯¸ì§€ì˜ ê°ì§€ ê²°ê³¼ ë¶„ì„
        for i, image in enumerate(self.test_images):
            detections = labeler._analyze_single_image(image)
            
            results["detection_counts"].append(len(detections))
            
            if detections:
                confidences = [d.confidence for d in detections]
                results["confidence_scores"].extend(confidences)
                
                avg_conf = np.mean(confidences)
                print(f"  ì´ë¯¸ì§€ {i+1}: {len(detections)}ê°œ ê°ì§€, í‰ê·  ì‹ ë¢°ë„ {avg_conf:.3f}")
            else:
                print(f"  ì´ë¯¸ì§€ {i+1}: ê°ì§€ ì—†ìŒ")
        
        # í†µê³„ ê³„ì‚°
        results["avg_confidence"] = np.mean(results["confidence_scores"]) if results["confidence_scores"] else 0.0
        
        print(f"ğŸ“Š ì •í™•ë„ ë¹„êµ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"  í‰ê·  ê°ì§€ ìˆ˜: {np.mean(results['detection_counts']):.1f}")
        print(f"  í‰ê·  ì‹ ë¢°ë„: {results['avg_confidence']:.3f}")
        
        return results
    
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 50)
        
        comprehensive_results = {
            "test_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "system_info": self._get_system_info(),
            "config": self.config
        }
        
        # 1. ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸
        comprehensive_results["speed_test"] = self.test_processing_speed()
        
        # 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
        comprehensive_results["memory_test"] = self.test_memory_usage()
        
        # 3. ìŠ¤ì¼€ì¼ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        comprehensive_results["scaling_test"] = self.test_scaling_performance()
        
        # 4. ì •í™•ë„ ë¹„êµ í…ŒìŠ¤íŠ¸
        comprehensive_results["accuracy_test"] = self.test_accuracy_comparison()
        
        # 5. ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
        comprehensive_results["performance_report"] = self.performance_optimizer.get_performance_report()
        
        # ê²°ê³¼ ì €ì¥
        self._save_results(comprehensive_results)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self._print_summary(comprehensive_results)
        
        return comprehensive_results
    
    def _get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        import sys
        return {
            "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
            "pytorch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "cuda_version": torch.version.cuda if torch.cuda.is_available() else "N/A",
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A"
        }
    
    def _save_results(self, results: Dict[str, Any]):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        output_dir = Path("data/outputs/performance")
        output_dir.mkdir(exist_ok=True)
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"performance_test_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {output_file}")
    
    def _print_summary(self, results: Dict[str, Any]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 50)
        print("ğŸ“Š ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 50)
        
        # ì²˜ë¦¬ ì†ë„
        speed_test = results["speed_test"]
        print(f"ğŸƒâ€â™‚ï¸ ì²˜ë¦¬ ì†ë„:")
        print(f"  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {speed_test['avg_processing_time']:.3f}ì´ˆ")
        print(f"  ì²˜ë¦¬ëŸ‰: {speed_test['throughput']:.2f} ì´ë¯¸ì§€/ì´ˆ")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_test = results["memory_test"]
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        print(f"  í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_test['avg_memory']:.1%}")
        print(f"  ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_test['peak_memory']:.1%}")
        
        # ì •í™•ë„
        accuracy_test = results["accuracy_test"]
        print(f"ğŸ¯ ì •í™•ë„:")
        print(f"  í‰ê·  ê°ì§€ ìˆ˜: {np.mean(accuracy_test['detection_counts']):.1f}")
        print(f"  í‰ê·  ì‹ ë¢°ë„: {accuracy_test['avg_confidence']:.3f}")
        
        # ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­
        performance_report = results["performance_report"]
        if "recommendations" in performance_report:
            print(f"ğŸ’¡ ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­:")
            for rec in performance_report["recommendations"]:
                print(f"  - {rec}")
        
        print("=" * 50)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ§ª Ultra Performance Test ì‹œì‘")
    
    # ì„±ëŠ¥ í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = PerformanceTester()
    
    # ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = tester.run_comprehensive_test()
    
    print("âœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
