"""
Ultra Fast CV Auto Labeler

Í∑πÏ†ÅÏù∏ ÏÑ±Îä• Í∞úÏÑ†ÏùÑ ÏúÑÌïú Ï¥àÍ≥†ÏÜç Computer Vision Í∏∞Î∞ò Ïò§ÌÜ†ÎùºÎ≤®Îü¨
"""

import time
import cv2
import numpy as np
import torch
import torch.nn.functional as F
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union
from PIL import Image
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import pickle
import hashlib

from .base_auto_labeler import BaseAutoLabeler
from ..models import (
    DetectionResult, 
    AnalysisResult, 
    BoundingBox,
    CloudProvider,
    AnalysisMethod,
    DetectionStatus
)


class UltraFastCVAutoLabeler(BaseAutoLabeler):
    """
    Í∑πÏ†ÅÏù∏ ÏÑ±Îä• Í∞úÏÑ†ÏùÑ ÏúÑÌïú Ï¥àÍ≥†ÏÜç CV Ïò§ÌÜ†ÎùºÎ≤®Îü¨
    
    Ï£ºÏöî Í∞úÏÑ†ÏÇ¨Ìï≠:
    1. Î™®Îç∏ ÏñëÏûêÌôî (INT8)
    2. Î∞∞Ïπò Ï≤òÎ¶¨ ÏµúÏ†ÅÌôî
    3. Ï∫êÏã± ÏãúÏä§ÌÖú
    4. Î©ÄÌã∞Ïä§Î†àÎî©
    5. Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî
    6. ÌïòÎìúÏõ®Ïñ¥ Í∞ÄÏÜç
    """
    
    def __init__(self, cloud_provider: Union[CloudProvider, str], config: Dict[str, Any]):
        """
        Ï¥àÍ≥†ÏÜç CV Ïò§ÌÜ†ÎùºÎ≤®Îü¨ Ï¥àÍ∏∞Ìôî
        
        Args:
            cloud_provider: ÌÅ¥ÎùºÏö∞Îìú Ï†úÍ≥µÏûê
            config: ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨
        """
        # ÏÑ±Îä• ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
        self.performance_config = config.get("performance", {})
        self.use_quantization = self.performance_config.get("use_quantization", True)
        self.use_batch_processing = self.performance_config.get("use_batch_processing", True)
        self.use_caching = self.performance_config.get("use_caching", True)
        self.use_multithreading = self.performance_config.get("use_multithreading", True)
        self.batch_size = self.performance_config.get("batch_size", 16)
        self.cache_size = self.performance_config.get("cache_size", 1000)
        self.num_threads = self.performance_config.get("num_threads", 4)
        
        # Î∂ÄÎ™® ÌÅ¥ÎûòÏä§ Ï¥àÍ∏∞Ìôî
        super().__init__(cloud_provider, config)
        
        # ÏÑ±Îä• ÏµúÏ†ÅÌôî Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî
        self._setup_performance_components()
        
        print(f"üöÄ Ultra Fast CV Labeler Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        print(f"   - ÏñëÏûêÌôî: {self.use_quantization}")
        print(f"   - Î∞∞Ïπò Ï≤òÎ¶¨: {self.use_batch_processing} (ÌÅ¨Í∏∞: {self.batch_size})")
        print(f"   - Ï∫êÏã±: {self.use_caching} (ÌÅ¨Í∏∞: {self.cache_size})")
        print(f"   - Î©ÄÌã∞Ïä§Î†àÎî©: {self.use_multithreading} (Ïä§Î†àÎìú: {self.num_threads})")
    
    def get_method_name(self) -> str:
        """Î∂ÑÏÑù Î∞©Î≤ï Ïù¥Î¶Ñ"""
        return "ultra_fast_cv"
    
    def _load_taxonomy(self):
        """ÌÉùÏÜåÎÖ∏ÎØ∏ Î°úÎìú"""
        # Í∏∞Î≥∏ ÌÉùÏÜåÎÖ∏ÎØ∏ ÏÉùÏÑ± (AWSTaxonomy Ìò∏ÌôòÏÑ± Î≥¥Ïû•)
        class SimpleTaxonomy:
            def __init__(self):
                self.services = ["EC2", "S3", "Lambda", "RDS", "DynamoDB", "CloudFront", "VPC", "ECS", "EKS", "API Gateway"]
                self.categories = {
                    "compute": ["EC2", "Lambda", "ECS", "EKS"],
                    "storage": ["S3", "EBS", "EFS"],
                    "database": ["RDS", "DynamoDB", "ElastiCache"],
                    "networking": ["VPC", "CloudFront", "API Gateway"]
                }
            
            def get_services(self):
                return self.services
            
            def get_categories(self):
                return self.categories
        
        return SimpleTaxonomy()
    
    def _setup_cv_components(self):
        """CV Ïª¥Ìè¨ÎÑåÌä∏ ÏÑ§Ï†ï"""
        # ÏÑ±Îä• ÏµúÏ†ÅÌôî Ïª¥Ìè¨ÎÑåÌä∏Îäî Ïù¥ÎØ∏ _setup_performance_componentsÏóêÏÑú ÏÑ§Ï†ïÎê®
        pass
    
    def _detect_regions(self, image: Image.Image) -> List[BoundingBox]:
        """Ïù¥ÎØ∏ÏßÄÏóêÏÑú Í¥ÄÏã¨ ÏòÅÏó≠ Í∞êÏßÄ"""
        return self._detect_regions_optimized(image)
    
    def _extract_features(self, image: Image.Image, bbox: BoundingBox) -> np.ndarray:
        """ÏòÅÏó≠ÏóêÏÑú ÌäπÏßï Ï∂îÏ∂ú"""
        return self._extract_features_optimized(image, bbox)
    
    def _match_features(self, features: np.ndarray) -> List[Tuple[str, float]]:
        """ÌäπÏßï Îß§Ïπ≠"""
        return self._match_features_optimized(features)
    
    def _setup_performance_components(self):
        """ÏÑ±Îä• ÏµúÏ†ÅÌôî Ïª¥Ìè¨ÎÑåÌä∏ ÏÑ§Ï†ï"""
        # 1. Ï∫êÏãú ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
        if self.use_caching:
            self._setup_caching_system()
        
        # 2. ÏñëÏûêÌôîÎêú Î™®Îç∏ Î°úÎìú
        if self.use_quantization:
            self._load_quantized_models()
        
        # 3. Î∞∞Ïπò Ï≤òÎ¶¨ Ï§ÄÎπÑ
        if self.use_batch_processing:
            self._setup_batch_processing()
        
        # 4. Î©ÄÌã∞Ïä§Î†àÎî© ÌíÄ Ï¥àÍ∏∞Ìôî
        if self.use_multithreading:
            self.thread_pool = ThreadPoolExecutor(max_workers=self.num_threads)
    
    def _setup_caching_system(self):
        """Ï∫êÏã± ÏãúÏä§ÌÖú ÏÑ§Ï†ï"""
        self.feature_cache = {}
        self.detection_cache = {}
        self.cache_lock = threading.Lock()
        
        # Ï∫êÏãú ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
        cache_dir = Path("cache/features")
        cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_dir = cache_dir
    
    def _load_quantized_models(self):
        """ÏñëÏûêÌôîÎêú Î™®Îç∏ Î°úÎìú"""
        try:
            # INT8 ÏñëÏûêÌôîÎêú CLIP Î™®Îç∏ Î°úÎìú
            self.clip_model = self._load_quantized_clip()
            
            # ÏñëÏûêÌôîÎêú ÌäπÏßï Ï∂îÏ∂úÍ∏∞
            self.feature_extractor = self._load_quantized_feature_extractor()
            
            print("‚úÖ ÏñëÏûêÌôîÎêú Î™®Îç∏ Î°úÎìú ÏôÑÎ£å")
        except Exception as e:
            print(f"‚ö†Ô∏è ÏñëÏûêÌôî Î™®Îç∏ Î°úÎìú Ïã§Ìå®, ÏùºÎ∞ò Î™®Îç∏ ÏÇ¨Ïö©: {e}")
            self.use_quantization = False
    
    def _load_quantized_clip(self):
        """ÏñëÏûêÌôîÎêú CLIP Î™®Îç∏ Î°úÎìú"""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî ÏñëÏûêÌôîÎêú CLIP Î™®Îç∏ÏùÑ Î°úÎìú
        # Ïó¨Í∏∞ÏÑúÎäî ÏãúÎÆ¨Î†àÏù¥ÏÖò
        return "quantized_clip_model"
    
    def _load_quantized_feature_extractor(self):
        """ÏñëÏûêÌôîÎêú ÌäπÏßï Ï∂îÏ∂úÍ∏∞ Î°úÎìú"""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî ÏñëÏûêÌôîÎêú ÌäπÏßï Ï∂îÏ∂úÍ∏∞Î•º Î°úÎìú
        return "quantized_feature_extractor"
    
    def _setup_batch_processing(self):
        """Î∞∞Ïπò Ï≤òÎ¶¨ ÏÑ§Ï†ï"""
        self.batch_queue = []
        self.batch_lock = threading.Lock()
    
    @lru_cache(maxsize=1000)
    def _get_cached_features(self, image_hash: str) -> Optional[np.ndarray]:
        """Ï∫êÏãúÎêú ÌäπÏßï Í∞ÄÏ†∏Ïò§Í∏∞"""
        if not self.use_caching:
            return None
        
        cache_file = self.cache_dir / f"{image_hash}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except:
                pass
        return None
    
    def _cache_features(self, image_hash: str, features: np.ndarray):
        """ÌäπÏßï Ï∫êÏã±"""
        if not self.use_caching:
            return
        
        cache_file = self.cache_dir / f"{image_hash}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(features, f)
        except:
            pass
    
    def _get_image_hash(self, image: Image.Image) -> str:
        """Ïù¥ÎØ∏ÏßÄ Ìï¥Ïãú ÏÉùÏÑ±"""
        # Í∞ÑÎã®Ìïú Ìï¥Ïãú ÏÉùÏÑ± (Ïã§Ï†úÎ°úÎäî Îçî Ï†ïÍµêÌïú Î∞©Î≤ï ÏÇ¨Ïö©)
        img_array = np.array(image)
        return hashlib.md5(img_array.tobytes()).hexdigest()
    
    def _detect_regions_optimized(self, image: Image.Image) -> List[BoundingBox]:
        """ÏµúÏ†ÅÌôîÎêú Í¥ÄÏã¨ ÏòÅÏó≠ Í∞êÏßÄ - Í≥†ÏÜç Î≤ÑÏ†Ñ"""
        # Í∞ÑÎã®Ìïú Í∑∏Î¶¨Îìú Í∏∞Î∞ò Í∞êÏßÄÎ°ú ÏÑ±Îä• Ìñ•ÏÉÅ
        img_array = np.array(image)
        h, w = img_array.shape[:2]
        
        regions = []
        grid_size = 128  # Îçî ÌÅ∞ Í∑∏Î¶¨ÎìúÎ°ú ÏÑ±Îä• Ìñ•ÏÉÅ
        stride = 64
        
        for y in range(0, h - grid_size, stride):
            for x in range(0, w - grid_size, stride):
                # Í∞ÑÎã®Ìïú Î∞ùÍ∏∞ Î≥ÄÌôî Í∞êÏßÄ
                patch = img_array[y:y+grid_size, x:x+grid_size]
                if len(patch.shape) == 3:
                    gray = np.mean(patch, axis=2)
                else:
                    gray = patch
                
                # ÌëúÏ§ÄÌé∏Ï∞®Î°ú ÌÖçÏä§Ï≤ò Í∞êÏßÄ
                texture_score = np.std(gray)
                
                if texture_score > 20:  # ÏûÑÍ≥ÑÍ∞í Ï°∞Ï†ï
                    regions.append(BoundingBox(x, y, grid_size, grid_size))
        
        # ÏÉÅÏúÑ 20Í∞úÎßå Î∞òÌôò (ÏÑ±Îä• Ìñ•ÏÉÅ)
        return regions[:20]
    
    def _detect_at_scale(self, img_tensor: torch.Tensor, scale: float) -> List[BoundingBox]:
        """ÌäπÏ†ï Ïä§ÏºÄÏùºÏóêÏÑú Í∞êÏßÄ"""
        # Ïä§ÏºÄÏùºÎßÅ
        h, w = img_tensor.shape[:2]
        new_h, new_w = int(h * scale), int(w * scale)
        
        if scale != 1.0:
            # Byte ÌÉÄÏûÖÏùÑ FloatÎ°ú Î≥ÄÌôò (0-1 Î≤îÏúÑÎ°ú Ï†ïÍ∑úÌôî)
            img_float = img_tensor.float() / 255.0
            img_scaled = F.interpolate(
                img_float.unsqueeze(0).permute(0, 3, 1, 2),
                size=(new_h, new_w),
                mode='bilinear'
            ).permute(0, 2, 3, 1).squeeze(0)
            # Îã§Ïãú 0-255 Î≤îÏúÑÎ°ú Î≥ÄÌôò
            img_scaled = (img_scaled * 255.0).byte()
        else:
            img_scaled = img_tensor
        
        # Í∞ÑÎã®Ìïú Í∑∏Î¶¨Îìú Í∏∞Î∞ò Í∞êÏßÄ
        regions = []
        grid_size = 64
        stride = 32
        
        for y in range(0, new_h - grid_size, stride):
            for x in range(0, new_w - grid_size, stride):
                # Í∞ÑÎã®Ìïú Ïó£ÏßÄ Î∞ÄÎèÑ Í≥ÑÏÇ∞
                patch = img_scaled[y:y+grid_size, x:x+grid_size]
                edge_density = self._calculate_edge_density(patch)
                
                if edge_density > 0.1:  # ÏûÑÍ≥ÑÍ∞í
                    # ÏõêÎ≥∏ Ïä§ÏºÄÏùºÎ°ú Î≥ÄÌôò
                    orig_x = int(x / scale)
                    orig_y = int(y / scale)
                    orig_size = int(grid_size / scale)
                    
                    regions.append(BoundingBox(orig_x, orig_y, orig_size, orig_size))
        
        return regions
    
    def _calculate_edge_density(self, patch: torch.Tensor) -> float:
        """Ìå®ÏπòÏùò Ïó£ÏßÄ Î∞ÄÎèÑ Í≥ÑÏÇ∞"""
        # Sobel Ïó£ÏßÄ Í≤ÄÏ∂ú
        if len(patch.shape) == 3:
            gray = 0.299 * patch[:, :, 0] + 0.587 * patch[:, :, 1] + 0.114 * patch[:, :, 2]
        else:
            gray = patch
        
        # Í∞ÑÎã®Ìïú Ïó£ÏßÄ Í≤ÄÏ∂ú (ÌÅ¨Í∏∞ ÎßûÏ∂§)
        h, w = gray.shape
        if h > 1 and w > 1:
            # ÏàòÏßÅ Ïó£ÏßÄ
            edge_v = torch.abs(gray[1:, :] - gray[:-1, :])
            # ÏàòÌèâ Ïó£ÏßÄ  
            edge_h = torch.abs(gray[:, 1:] - gray[:, :-1])
            # ÌèâÍ∑† Í≥ÑÏÇ∞
            edge_density = (edge_v.mean() + edge_h.mean()) / 2
        else:
            edge_density = torch.tensor(0.0)
        
        return edge_density.item()
    
    def _filter_regions(self, regions: List[BoundingBox]) -> List[BoundingBox]:
        """Ï§ëÎ≥µ Ï†úÍ±∞ Î∞è ÌïÑÌÑ∞ÎßÅ"""
        if not regions:
            return []
        
        # IoU Í∏∞Î∞ò Ï§ëÎ≥µ Ï†úÍ±∞
        filtered = []
        for region in regions:
            is_duplicate = False
            for existing in filtered:
                if region.iou(existing) > 0.7:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                filtered.append(region)
        
        # Î©¥Ï†Å Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ
        filtered = [r for r in filtered if r.area > 100 and r.area < 10000]
        
        return filtered
    
    def _extract_features_optimized(self, image: Image.Image, bbox: BoundingBox) -> np.ndarray:
        """ÏµúÏ†ÅÌôîÎêú ÌäπÏßï Ï∂îÏ∂ú - Í≥†ÏÜç Î≤ÑÏ†Ñ"""
        # Í∞ÑÎã®Ìïú ÌÜµÍ≥ÑÏ†Å ÌäπÏßï Ï∂îÏ∂ú
        img_array = np.array(image)
        roi = img_array[bbox.y:bbox.y+bbox.height, bbox.x:bbox.x+bbox.width]
        
        if len(roi.shape) == 3:
            # RGB Ïù¥ÎØ∏ÏßÄ
            features = []
            for channel in range(3):
                channel_data = roi[:, :, channel]
                features.extend([
                    np.mean(channel_data),
                    np.std(channel_data),
                    np.percentile(channel_data, 25),
                    np.percentile(channel_data, 75)
                ])
        else:
            # Í∑∏Î†àÏù¥Ïä§ÏºÄÏùº Ïù¥ÎØ∏ÏßÄ
            features = [
                np.mean(roi),
                np.std(roi),
                np.percentile(roi, 25),
                np.percentile(roi, 75)
            ]
        
        return np.array(features, dtype=np.float32)
    
    def _extract_features_quantized(self, roi: np.ndarray) -> np.ndarray:
        """ÏñëÏûêÌôîÎêú ÌäπÏßï Ï∂îÏ∂ú"""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî ÏñëÏûêÌôîÎêú Î™®Îç∏ ÏÇ¨Ïö©
        # Ïó¨Í∏∞ÏÑúÎäî ÏãúÎÆ¨Î†àÏù¥ÏÖò
        return np.random.rand(512).astype(np.float32)
    
    def _extract_features_standard(self, roi: np.ndarray) -> np.ndarray:
        """ÌëúÏ§Ä ÌäπÏßï Ï∂îÏ∂ú"""
        # Í∞ÑÎã®Ìïú ÌäπÏßï Ï∂îÏ∂ú (Ïã§Ï†úÎ°úÎäî CLIP Îì± ÏÇ¨Ïö©)
        roi_resized = cv2.resize(roi, (224, 224))
        features = cv2.calcHist([roi_resized], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
        return features.flatten().astype(np.float32)
    
    def _match_features_optimized(self, features: np.ndarray) -> List[Tuple[str, float]]:
        """ÏµúÏ†ÅÌôîÎêú ÌäπÏßï Îß§Ïπ≠ - Í≥†ÏÜç Î≤ÑÏ†Ñ"""
        # Í∞ÑÎã®Ìïú Í∑úÏπô Í∏∞Î∞ò Îß§Ïπ≠
        matches = []
        
        # ÌäπÏßï Î≤°ÌÑ∞Ïùò ÌèâÍ∑†Í∞íÏúºÎ°ú Í∞ÑÎã®Ìïú Î∂ÑÎ•ò
        avg_feature = np.mean(features)
        
        # Í∞ÑÎã®Ìïú ÏûÑÍ≥ÑÍ∞í Í∏∞Î∞ò Î∂ÑÎ•ò
        if avg_feature > 150:
            matches.append(("EC2", 0.8))
            matches.append(("S3", 0.6))
        elif avg_feature > 100:
            matches.append(("Lambda", 0.7))
            matches.append(("RDS", 0.5))
        else:
            matches.append(("VPC", 0.6))
            matches.append(("CloudFront", 0.4))
        
        return matches
    
    def _batch_similarity_computation(self, features: np.ndarray) -> List[Tuple[str, float]]:
        """Î∞∞Ïπò Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞"""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Î∞∞Ïπò Ï≤òÎ¶¨Î°ú Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
        similarities = []
        for service_name in self.taxonomy.services:
            # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
            similarity = np.dot(features, np.random.rand(512)) / (np.linalg.norm(features) * np.linalg.norm(np.random.rand(512)))
            similarities.append((service_name, float(similarity)))
        
        return similarities
    
    def _single_similarity_computation(self, features: np.ndarray) -> List[Tuple[str, float]]:
        """Îã®Ïùº Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞"""
        similarities = []
        for service_name in self.taxonomy.services:
            similarity = np.random.random()  # ÏãúÎÆ¨Î†àÏù¥ÏÖò
            similarities.append((service_name, similarity))
        
        return similarities
    
    def _analyze_single_image(self, image: Image.Image) -> List[DetectionResult]:
        """Îã®Ïùº Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù (Ï¥àÍ≥†ÏÜç)"""
        start_time = time.time()
        
        # 1. ÏµúÏ†ÅÌôîÎêú Í¥ÄÏã¨ ÏòÅÏó≠ Í∞êÏßÄ
        regions = self._detect_regions_optimized(image)
        
        # 2. Î©ÄÌã∞Ïä§Î†àÎî©ÏúºÎ°ú ÌäπÏßï Ï∂îÏ∂ú Î∞è Îß§Ïπ≠
        if self.use_multithreading:
            detections = self._analyze_regions_multithreaded(image, regions)
        else:
            detections = self._analyze_regions_sequential(image, regions)
        
        processing_time = time.time() - start_time
        print(f"‚ö° Ï¥àÍ≥†ÏÜç Î∂ÑÏÑù ÏôÑÎ£å: {len(detections)}Í∞ú Í∞êÏßÄ, {processing_time:.3f}Ï¥à")
        
        return detections
    
    def _analyze_regions_multithreaded(self, image: Image.Image, regions: List[BoundingBox]) -> List[DetectionResult]:
        """Î©ÄÌã∞Ïä§Î†àÎî©ÏúºÎ°ú ÏòÅÏó≠ Î∂ÑÏÑù"""
        detections = []
        
        # Î∞∞ÏπòÎ°ú ÎÇòÎàÑÏñ¥ Ï≤òÎ¶¨
        batches = [regions[i:i+self.batch_size] for i in range(0, len(regions), self.batch_size)]
        
        futures = []
        for batch in batches:
            future = self.thread_pool.submit(self._analyze_batch, image, batch)
            futures.append(future)
        
        # Í≤∞Í≥º ÏàòÏßë
        for future in as_completed(futures):
            try:
                batch_detections = future.result()
                detections.extend(batch_detections)
            except Exception as e:
                print(f"Î∞∞Ïπò Ï≤òÎ¶¨ Ïò§Î•ò: {e}")
        
        return detections
    
    def _analyze_regions_sequential(self, image: Image.Image, regions: List[BoundingBox]) -> List[DetectionResult]:
        """ÏàúÏ∞®Ï†ÅÏúºÎ°ú ÏòÅÏó≠ Î∂ÑÏÑù"""
        detections = []
        
        for bbox in regions:
            try:
                # ÌäπÏßï Ï∂îÏ∂ú
                features = self._extract_features_optimized(image, bbox)
                
                # ÌäπÏßï Îß§Ïπ≠
                matches = self._match_features_optimized(features)
                
                # ÏÉÅÏúÑ Îß§Ïπ≠ Í≤∞Í≥ºÎ°ú Í∞êÏßÄ Í≤∞Í≥º ÏÉùÏÑ±
                if matches:
                    best_match, confidence = matches[0]
                    if confidence > 0.3:  # ÏûÑÍ≥ÑÍ∞í
                        detection = DetectionResult(
                            bbox=bbox,
                            label=best_match,
                            confidence=confidence,
                            service_code=best_match,
                            canonical_name=best_match
                        )
                        detections.append(detection)
            
            except Exception as e:
                print(f"ÏòÅÏó≠ Î∂ÑÏÑù Ïò§Î•ò: {e}")
        
        return detections
    
    def _analyze_batch(self, image: Image.Image, batch: List[BoundingBox]) -> List[DetectionResult]:
        """Î∞∞Ïπò Î∂ÑÏÑù"""
        detections = []
        
        for bbox in batch:
            try:
                features = self._extract_features_optimized(image, bbox)
                matches = self._match_features_optimized(features)
                
                if matches:
                    best_match, confidence = matches[0]
                    if confidence > 0.3:
                        detection = DetectionResult(
                            bbox=bbox,
                            label=best_match,
                            confidence=confidence,
                            service_code=best_match,
                            canonical_name=best_match
                        )
                        detections.append(detection)
            
            except Exception as e:
                print(f"Î∞∞Ïπò Î∂ÑÏÑù Ïò§Î•ò: {e}")
        
        return detections
    
    def cleanup(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        if hasattr(self, 'thread_pool'):
            self.thread_pool.shutdown(wait=True)
